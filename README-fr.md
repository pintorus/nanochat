# nanochat

![nanochat logo](dev/nanochat.png)

> Le meilleur ChatGPT que 100 $ peuvent acheter.

Ce dÃ©pÃ´t est une implÃ©mentation complÃ¨te d'un LLM comme ChatGPT dans une base de code unique, claire, minimale, modifiable et avec peu de dÃ©pendances. nanochat est conÃ§u pour fonctionner sur un seul nÅ“ud 8XH100 via des scripts comme [speedrun.sh](speedrun.sh), qui exÃ©cutent l'intÃ©gralitÃ© du pipeline du dÃ©but Ã  la fin. Cela inclut la tokenisation, le prÃ©-entraÃ®nement, l'ajustement fin, l'Ã©valuation, l'infÃ©rence et le service web via une interface utilisateur simple pour que vous puissiez parler Ã  votre propre LLM comme avec ChatGPT. nanochat deviendra le projet phare du cours LLM101n dÃ©veloppÃ© par Eureka Labs.

## Parlez-lui

Pour avoir une idÃ©e du rÃ©sultat final de ce dÃ©pÃ´t, vous pouvez actuellement trouver [nanochat d32](https://github.com/karpathy/nanochat/discussions/8) hÃ©bergÃ© sur [nanochat.karpathy.ai](https://nanochat.karpathy.ai/). "d32" signifie que ce modÃ¨le possÃ¨de 32 couches dans le rÃ©seau de neurones Transformer. Ce modÃ¨le compte 1,9 milliard de paramÃ¨tres, il a Ã©tÃ© entraÃ®nÃ© sur 38 milliards de tokens en exÃ©cutant simplement le script unique [run1000.sh](run1000.sh), et le coÃ»t total d'entraÃ®nement Ã©tait d'environ 800 $ (environ 33 heures de temps d'entraÃ®nement sur un nÅ“ud GPU 8XH100). Bien qu'aujourd'hui cela suffise Ã  surpasser GPT-2 de 2019, cela reste trÃ¨s en deÃ§Ã  des grands modÃ¨les de langage modernes comme GPT-5. En parlant Ã  ces micro-modÃ¨les, vous verrez qu'ils font beaucoup d'erreurs, ils sont un peu naÃ¯fs et idiots et ils hallucinent Ã©normÃ©ment, un peu comme des enfants. C'est plutÃ´t amusant. Mais ce qui rend nanochat unique, c'est qu'il est entiÃ¨rement Ã  vous - entiÃ¨rement configurable, ajustable, modifiable et entraÃ®nÃ© par vous du dÃ©but Ã  la fin. Pour entraÃ®ner et parler au vÃ´tre, nous nous tournons vers...

## DÃ©marrage rapide

Le moyen le plus rapide de ressentir la magie est d'exÃ©cuter le script speedrun [speedrun.sh](speedrun.sh), qui entraÃ®ne et effectue l'infÃ©rence du niveau 100 $ de nanochat. Sur un nÅ“ud 8XH100 Ã  24 $/h, cela donne un temps d'exÃ©cution total d'environ 4 heures. DÃ©marrez une nouvelle machine GPU 8XH100 auprÃ¨s de votre fournisseur prÃ©fÃ©rÃ© (par exemple, j'utilise et j'aime [Lambda](https://lambda.ai/service/gpu-cloud)), et lancez le script d'entraÃ®nement :

```bash
bash speedrun.sh
```

Alternativement, comme le script s'exÃ©cute pendant 4 heures, j'aime le lancer comme ceci dans une nouvelle session screen `speedrun` (et Ã©galement enregistrer la sortie dans `speedrun.log`) :

```bash
screen -L -Logfile speedrun.log -S speedrun bash speedrun.sh
```

Consultez l'[aide-mÃ©moire screen](https://gist.github.com/jctosta/af918e1618682638aa82) si vous Ãªtes moins familier. Vous pouvez le regarder fonctionner dans la session screen, ou vous dÃ©tacher avec `Ctrl-a d` et `tail speedrun.log` pour voir la progression. Maintenant, attendez 4 heures. Une fois terminÃ©, vous pouvez parler Ã  votre LLM via l'interface web semblable Ã  ChatGPT. Assurez-vous Ã  nouveau que votre environnement virtuel uv local est actif (exÃ©cutez `source .venv/bin/activate`), et servez-le :

```bash
python -m scripts.chat_web
```

Puis visitez l'URL affichÃ©e. Assurez-vous d'y accÃ©der correctement, par exemple sur Lambda utilisez l'IP publique du nÅ“ud sur lequel vous Ãªtes, suivie du port, donc par exemple [http://209.20.xxx.xxx:8000/](http://209.20.xxx.xxx:8000/), etc. Ensuite, parlez Ã  votre LLM comme vous parleriez normalement Ã  ChatGPT ! Demandez-lui d'Ã©crire des histoires ou des poÃ¨mes. Demandez-lui de vous dire qui vous Ãªtes pour voir une hallucination. Demandez-lui pourquoi le ciel est bleu. Ou pourquoi il est vert. Le speedrun est un modÃ¨le de capacitÃ© 4e19 FLOPs donc c'est un peu comme parler Ã  un enfant de maternelle :).

---

<img width="2672" height="1520" alt="image" src="https://github.com/user-attachments/assets/ed39ddf8-2370-437a-bedc-0f39781e76b5" />

---

Vous pouvez Ã©galement `cat report.md`, fichier qui est apparu dans le rÃ©pertoire du projet et contient le "bulletin de notes" de l'exÃ©cution, c'est-Ã -dire un ensemble d'Ã©valuations et de mÃ©triques. Ã€ la toute fin, vous verrez un tableau rÃ©capitulatif, par exemple :

---

- CaractÃ¨res : 333 989
- Lignes : 8 304
- Fichiers : 44
- Tokens (approx) : 83 497
- DÃ©pendances (lignes uv.lock) : 2 004

| MÃ©trique        | BASE     | MID      | SFT      | RL       |
|-----------------|----------|----------|----------|----------|
| CORE            | 0.2219   | -        | -        | -        |
| ARC-Challenge   | -        | 0.2875   | 0.2807   | -        |
| ARC-Easy        | -        | 0.3561   | 0.3876   | -        |
| GSM8K           | -        | 0.0250   | 0.0455   | 0.0758   |
| HumanEval       | -        | 0.0671   | 0.0854   | -        |
| MMLU            | -        | 0.3111   | 0.3151   | -        |
| ChatCORE        | -        | 0.0730   | 0.0884   | -        |

Temps d'horloge total : 3h51m

---

(Votre tableau pourrait manquer le numÃ©ro RL par dÃ©faut). Pour beaucoup plus d'informations sur le script speedrun et ce qu'il faut rechercher et attendre, veuillez vous rÃ©fÃ©rer Ã  la prÃ©sentation que j'ai publiÃ©e dans les Discussions du dÃ©pÃ´t : ["Introducing nanochat: The best ChatGPT that $100 can buy"](https://github.com/karpathy/nanochat/discussions/1).

## ModÃ¨les plus grands

Sans surprise, 100 $ ne suffisent pas pour entraÃ®ner un clone ChatGPT trÃ¨s performant. En fait, les LLM sont cÃ©lÃ¨bres pour leurs dÃ©penses d'investissement de plusieurs millions de dollars. Pour nos besoins, je pense qu'il y a deux autres Ã©chelles intÃ©ressantes. La premiÃ¨re est le modÃ¨le d26 de niveau ~300 $ (c'est-Ã -dire depth=26) qui s'entraÃ®ne en ~12 heures, qui surpasse lÃ©gÃ¨rement le score CORE de GPT-2. La seconde est le niveau 100 $ (~41,6 heures), juste parce que c'est un nombre rond agrÃ©able. Mais ces deux niveaux ne sont pas encore entiÃ¨rement pris en charge et ne sont donc pas encore attachÃ©s ici dans la branche master.

Cela dit, pour donner une idÃ©e, les exemples de modifications nÃ©cessaires pour le fichier [speedrun.sh](speedrun.sh) pour entraÃ®ner un modÃ¨le d26 de niveau GPT-2 n'impliquent que trois changements :

```bash
...
# vous devrez tÃ©lÃ©charger plus de fragments de donnÃ©es pour le prÃ©-entraÃ®nement
# obtenez le nombre de paramÃ¨tres, multipliez par 20 pour obtenir les tokens, multipliez par 4,8 pour obtenir les caractÃ¨res,
# divisez par 250 millions pour obtenir le nombre de fragments. todo besoin d'amÃ©liorer cela...
python -m nanochat.dataset -n 450 &
...
# utilisez --depth pour augmenter la taille du modÃ¨le. pour ne pas manquer de mÃ©moire, divisez par deux la taille du lot de pÃ©riphÃ©rique 32 -> 16 :
torchrun --standalone --nproc_per_node=8 -m scripts.base_train -- --depth=26 --device_batch_size=16
...
# assurez-vous d'utiliser le mÃªme plus tard pendant le midtraining :
torchrun --standalone --nproc_per_node=8 -m scripts.mid_train -- --device_batch_size=16
```

C'est tout ! La chose la plus importante Ã  laquelle il faut prÃªter attention est de s'assurer que vous avez suffisamment de fragments de donnÃ©es pour vous entraÃ®ner (sinon le code bouclera et fera plus d'Ã©poques sur le mÃªme ensemble d'entraÃ®nement, diminuant un peu la vitesse d'apprentissage), et de gÃ©rer votre mÃ©moire/VRAM, principalement en diminuant le `device_batch_size` jusqu'Ã  ce que les choses rentrent (les scripts compensent automatiquement en augmentant le nombre de boucles d'accumulation de gradient, transformant simplement le calcul parallÃ¨le en calcul sÃ©quentiel).

Et un peu plus sur les environnements informatiques qui exÃ©cuteront nanochat :

- Le code fonctionnera trÃ¨s bien sur le nÅ“ud GPU Ampere 8XA100 Ã©galement, mais un peu plus lentement.
- Tout le code fonctionnera trÃ¨s bien mÃªme sur un seul GPU en omettant `torchrun`, et produira des rÃ©sultats ~identiques (le code basculera automatiquement vers l'accumulation de gradient), mais vous devrez attendre 8 fois plus longtemps.
- Si votre/vos GPU ont moins de 80 Go, vous devrez ajuster certains des hyperparamÃ¨tres ou vous manquerez de mÃ©moire / VRAM. Recherchez `--device_batch_size` dans les scripts et rÃ©duisez-le jusqu'Ã  ce que les choses rentrent. Par exemple de 32 (par dÃ©faut) Ã  16, 8, 4, 2, ou mÃªme 1. Moins que cela, vous devrez en savoir un peu plus sur ce que vous faites et devenir plus crÃ©atif.
- La plupart du code est du PyTorch assez standard donc il devrait fonctionner sur tout ce qui le supporte - xpu, mps, ou etc, mais je ne l'ai pas implÃ©mentÃ© prÃªt Ã  l'emploi donc cela pourrait nÃ©cessiter un peu de bricolage.

## ExÃ©cution sur CPU / MPS

nanochat peut Ãªtre exÃ©cutÃ© sur CPU ou sur MPS (si vous Ãªtes sur Macbook), et essaiera automatiquement de dÃ©tecter quel pÃ©riphÃ©rique est le meilleur pour l'exÃ©cution. Vous n'irez pas trÃ¨s loin sans GPU, mais au moins vous pourrez exÃ©cuter les chemins de code et peut-Ãªtre entraÃ®ner un petit LLM avec un peu de patience. Pour un exemple de comment rendre toutes les commandes d'exÃ©cution beaucoup plus petites (n'hÃ©sitez pas Ã  ajuster !), vous pouvez vous rÃ©fÃ©rer au fichier [dev/runcpu.sh](dev/runcpu.sh). Vous verrez que je restreins essentiellement tous les scripts pour entraÃ®ner des modÃ¨les plus petits, pour s'exÃ©cuter pendant un nombre d'itÃ©rations plus court, etc. Cette fonctionnalitÃ© est nouvelle, un peu complexe (a touchÃ© beaucoup de code), et a Ã©tÃ© fusionnÃ©e dans cette [CPU|MPS PR](https://github.com/karpathy/nanochat/pull/88) le 21 octobre 2025.

## Personnalisation

Pour personnaliser votre nanochat, consultez [Guide: infusing identity to your nanochat](https://github.com/karpathy/nanochat/discussions/139) dans les Discussions, qui dÃ©crit comment vous pouvez ajuster la personnalitÃ© de votre nanochat grÃ¢ce Ã  la gÃ©nÃ©ration de donnÃ©es synthÃ©tiques et au mÃ©lange de ces donnÃ©es dans les Ã©tapes de midtraining et SFT.

De plus, pour ajouter de nouvelles capacitÃ©s Ã  nanochat, consultez [Guide: counting r in strawberry (and how to add abilities generally)](https://github.com/karpathy/nanochat/discussions/164).

## Questions

nanochat est conÃ§u pour Ãªtre court et agrÃ©able. Un grand avantage de cela est que nous pouvons empaqueter tous les fichiers ensemble et les copier-coller dans votre LLM prÃ©fÃ©rÃ© pour poser des questions arbitraires. Par exemple, j'aime empaqueter le dÃ©pÃ´t en utilisant l'utilitaire [files-to-prompt](https://github.com/simonw/files-to-prompt) comme ceci :

```bash
files-to-prompt . -e py -e md -e rs -e html -e toml -e sh --ignore "*target*" --cxml > packaged.txt
```

Cela inclut tous les fichiers py, rs, html, toml, sh, exclut le dossier `rustbpe/target`, et choisit le format de sortie cxml. Tout est Ã©crit dans le fichier `packaged.txt`, qui mesure actuellement ~330 Ko (c'est-Ã -dire bien en dessous de ~100 K tokens pour un LLM de pointe), et ~8 K lignes de code dans 45 fichiers.

Alternativement, je recommande d'utiliser [DeepWiki](https://deepwiki.com/karpathy/nanochat) de Devin/Cognition pour poser des questions sur ce dÃ©pÃ´t. Dans l'URL de ce dÃ©pÃ´t, changez simplement github.com en deepwiki.com, et c'est parti.

## Tests

Je n'ai pas trop investi ici mais quelques tests existent, en particulier pour le tokenizer. ExÃ©cutez par exemple comme :

```bash
python -m pytest tests/test_rustbpe.py -v -s
```

## Structure des fichiers

```
.
â”œâ”€â”€ LICENSE
â”œâ”€â”€ README.md
â”œâ”€â”€ dev
â”‚   â”œâ”€â”€ gen_synthetic_data.py       # Exemple de donnÃ©es synthÃ©tiques pour l'identitÃ©
â”‚   â”œâ”€â”€ generate_logo.html
â”‚   â”œâ”€â”€ nanochat.png
â”‚   â”œâ”€â”€ repackage_data_reference.py # GÃ©nÃ©ration de fragments de donnÃ©es de prÃ©-entraÃ®nement
â”‚   â””â”€â”€ runcpu.sh                   # Petit exemple de comment exÃ©cuter sur CPU/MPS
â”œâ”€â”€ nanochat
â”‚   â”œâ”€â”€ __init__.py                 # vide
â”‚   â”œâ”€â”€ adamw.py                    # Optimiseur AdamW distribuÃ©
â”‚   â”œâ”€â”€ checkpoint_manager.py       # Sauvegarde/Chargement des points de contrÃ´le de modÃ¨le
â”‚   â”œâ”€â”€ common.py                   # Divers petits utilitaires, qualitÃ© de vie
â”‚   â”œâ”€â”€ configurator.py             # Une alternative supÃ©rieure Ã  argparse
â”‚   â”œâ”€â”€ core_eval.py                # Ã‰value le score CORE du modÃ¨le de base (article DCLM)
â”‚   â”œâ”€â”€ dataloader.py               # Chargeur de donnÃ©es distribuÃ© avec tokenisation
â”‚   â”œâ”€â”€ dataset.py                  # Utilitaires de tÃ©lÃ©chargement/lecture pour les donnÃ©es de prÃ©-entraÃ®nement
â”‚   â”œâ”€â”€ engine.py                   # InfÃ©rence de modÃ¨le efficace avec KV Cache
â”‚   â”œâ”€â”€ execution.py                # Permet au LLM d'exÃ©cuter du code Python comme outil
â”‚   â”œâ”€â”€ gpt.py                      # Le nn.Module Transformer GPT
â”‚   â”œâ”€â”€ logo.svg
â”‚   â”œâ”€â”€ loss_eval.py                # Ã‰value les bits par octet (au lieu de la perte)
â”‚   â”œâ”€â”€ muon.py                     # Optimiseur Muon distribuÃ©
â”‚   â”œâ”€â”€ report.py                   # Utilitaires pour Ã©crire le rapport nanochat
â”‚   â”œâ”€â”€ tokenizer.py                # Wrapper de tokenizer BPE dans le style de GPT-4
â”‚   â””â”€â”€ ui.html                     # HTML/CSS/JS pour l'interface nanochat
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ run1000.sh                      # EntraÃ®ne le nanochat d32 ~800 $
â”œâ”€â”€ rustbpe                         # EntraÃ®neur de tokenizer BPE Rust personnalisÃ©
â”‚   â”œâ”€â”€ Cargo.lock
â”‚   â”œâ”€â”€ Cargo.toml
â”‚   â”œâ”€â”€ README.md                   # voir pourquoi cela existe mÃªme
â”‚   â””â”€â”€ src
â”‚       â””â”€â”€ lib.rs
â”œâ”€â”€ scripts
â”‚   â”œâ”€â”€ base_eval.py                # ModÃ¨le de base : calculer le score CORE
â”‚   â”œâ”€â”€ base_loss.py                # ModÃ¨le de base : calculer les bits par octet, Ã©chantillon
â”‚   â”œâ”€â”€ base_train.py               # ModÃ¨le de base : entraÃ®ner
â”‚   â”œâ”€â”€ chat_cli.py                 # ModÃ¨le de chat (SFT/Mid) : parler via CLI
â”‚   â”œâ”€â”€ chat_eval.py                # ModÃ¨le de chat (SFT/Mid) : tÃ¢ches d'Ã©valuation
â”‚   â”œâ”€â”€ chat_rl.py                  # ModÃ¨le de chat (SFT/Mid) : apprentissage par renforcement
â”‚   â”œâ”€â”€ chat_sft.py                 # ModÃ¨le de chat : entraÃ®ner SFT
â”‚   â”œâ”€â”€ chat_web.py                 # ModÃ¨le de chat (SFT/Mid) : parler via WebUI
â”‚   â”œâ”€â”€ mid_train.py                # ModÃ¨le de chat : midtraining
â”‚   â”œâ”€â”€ tok_eval.py                 # Tokenizer : Ã©valuer le taux de compression
â”‚   â””â”€â”€ tok_train.py                # Tokenizer : l'entraÃ®ner
â”œâ”€â”€ speedrun.sh                     # EntraÃ®ne le nanochat d20 ~100 $
â”œâ”€â”€ tasks
â”‚   â”œâ”€â”€ arc.py                      # Questions scientifiques Ã  choix multiples
â”‚   â”œâ”€â”€ common.py                   # TaskMixture | TaskSequence
â”‚   â”œâ”€â”€ customjson.py               # CrÃ©er une tÃ¢che Ã  partir de conversations jsonl arbitraires
â”‚   â”œâ”€â”€ gsm8k.py                    # 8K questions de mathÃ©matiques de l'Ã©cole primaire
â”‚   â”œâ”€â”€ humaneval.py                # Nom impropre ; TÃ¢che de codage Python simple
â”‚   â”œâ”€â”€ mmlu.py                     # Questions Ã  choix multiples, sujets variÃ©s
â”‚   â”œâ”€â”€ smoltalk.py                 # Ensemble de donnÃ©es conglomÃ©rat de SmolTalk de HF
â”‚   â””â”€â”€ spellingbee.py              # TÃ¢che enseignant au modÃ¨le Ã  Ã©peler/compter les lettres
â”œâ”€â”€ tests
â”‚   â””â”€â”€ test_rustbpe.py
â””â”€â”€ uv.lock
```

## Contribution

nanochat est loin d'Ãªtre terminÃ©. L'objectif est d'amÃ©liorer l'Ã©tat de l'art en matiÃ¨re de micro-modÃ¨les accessibles pour travailler de bout en bout avec des budgets de < 1000 dollars. L'accessibilitÃ© concerne le coÃ»t global mais aussi la complexitÃ© cognitive - nanochat n'est pas un "framework" LLM exhaustivement configurable ; il n'y aura pas d'objets de configuration gÃ©ants, de fabriques de modÃ¨les ou de monstres if-then-else dans la base de code. C'est une base de code unique, cohÃ©rente, minimale, lisible, modifiable, maximalement forkable "base de rÃ©fÃ©rence solide" conÃ§ue pour s'exÃ©cuter du dÃ©but Ã  la fin et produire un clone ChatGPT concret et son bulletin de notes.

Politique LLM actuelle : divulgation. Lors de la soumission d'une PR, veuillez dÃ©clarer toutes les parties qui ont eu une contribution substantielle de LLM et que vous n'avez pas Ã©crites ou que vous ne comprenez pas entiÃ¨rement.

## Remerciements

- Le nom (nanochat) dÃ©rive de mon projet antÃ©rieur [nanoGPT](https://github.com/karpathy/nanoGPT), qui ne couvrait que le prÃ©-entraÃ®nement.
- nanochat est Ã©galement inspirÃ© par [modded-nanoGPT](https://github.com/KellerJordan/modded-nanogpt), qui a gamifiÃ© le dÃ©pÃ´t nanoGPT avec des mÃ©triques claires et un classement, et emprunte beaucoup de ses idÃ©es et une partie de son implÃ©mentation pour le prÃ©-entraÃ®nement.
- Merci Ã  [HuggingFace](https://huggingface.co/) pour fineweb et smoltalk.
- Merci [Lambda](https://lambda.ai/service/gpu-cloud) pour le calcul utilisÃ© dans le dÃ©veloppement de ce projet.
- Merci au chef chuchoteur de LLM ğŸ§™â€â™‚ï¸ Alec Radford pour ses conseils/orientations.

## Citation

Si vous trouvez nanochat utile dans votre recherche, citez simplement comme :

```bibtex
@misc{nanochat,
  author = {Andrej Karpathy},
  title = {nanochat: The best ChatGPT that $100 can buy},
  year = {2025},
  publisher = {GitHub},
  url = {https://github.com/karpathy/nanochat}
}
```

## Licence

MIT